---
title: "hw5"
output: html_document
date: '2022-05-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
library(tidyverse)
library(tidymodels)
library(janitor)
library(glmnet)
set.seed(42)
```


## Q1

```{r}
data<- read_csv("data/Pokemon.csv")%>%clean_names()
```

The clean_names() function can returns names with only lowercase letters, with _ as a separator; and handles special characters and spaces.

This is useful because it standardized the variable name, and make columns access by variable name easily.


## Q2

```{r}
ggplot(data=data, aes(x=type_1)) +
  geom_bar(stat="count") +
  coord_flip()
```

There are 18 classes of Type 1. Flying type has very few Pokemon.


```{r}
data <- data %>% filter((type_1 == "Bug" | type_1 == "Fire" |
                           type_1 == "Grass" | type_1 == "Normal" |
                           type_1 == "Water" | type_1 == "Psychic"))

data$type_1 <- as.factor(data$type_1)
data$legendary <- as.factor(data$legendary)
data$generation <- as.factor(data$generation)
```


## Q3

```{r}
split = initial_split(data, prob = .8,strata = type_1) 

poke_train = training(split)
poke_test = testing(split)

folds = vfold_cv(poke_train, v = 5, strata = type_1)
```

We stratify the folds and the training/testing data to make sure that the models we train on and fit to the data are representative of the true distribution. 
Which can avoid larger difference between performance of training and validation.

## Q4

```{r}
poke_recipe = recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = poke_train) %>%
  step_dummy(c("legendary", "generation")) %>% 
  step_normalize(all_predictors())
```

## Q5

```{r}
elastic_net = multinom_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet") %>%
  set_mode("classification")

poke_workflow = workflow() %>% 
  add_model(elastic_net) %>% 
  add_recipe(poke_recipe)

```


```{r}
poke_grid = grid_regular(penalty(range=c(-5, 5)), mixture(range=c(0, 1)), levels=10)
```

Will be fitting 500 models.

## Q6

```{r}
poke_tune = poke_workflow %>% tune_grid(
    resamples = folds, 
    grid = poke_grid)
```


```{r}
autoplot(poke_tune)
```

The lower penalty is, the better result it have.


## Q7


```{r}
poke_best=select_best(poke_tune, metric = "roc_auc")
```

```{r}
poke_model = poke_workflow %>%
  finalize_workflow(poke_best) %>% 
  fit(poke_train) %>% 
  augment(poke_test) 

accuracy(poke_model, truth = type_1, estimate = .pred_class)
```



## Q8

```{r}
all_roc_auc=roc_auc(poke_model, truth = type_1, .pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic)

all_roc_auc
```


```{r}

roc_curves <- roc_curve(poke_model, truth = type_1, .pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic)%>%
  autoplot()
  
roc_curves
```

```{r}
final_model_conf <-conf_mat(poke_model,truth = type_1, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
final_model_conf
```

We can say that, the performance of the model is very poor. We can tell that the normal and bug are well predicted. But model is not good at predictiing water , grass, and fire type.

The reason is that we have poor date sample which do not have enough useful features per sample. Therefore, we need more samples.


## Q9

```{r}
shot = c(rep(0,time = 464),rep(1,time=337))

n=1000


N = length(shot)

result = numeric(n)

for (i in 1:n) { 
  samp <- sample(shot, replace=TRUE) 
  result[i] = mean(samp) #calculate and save mean of that sample
}

hist(result)

quantile(result,probs = c(0.01,0.99))
```


